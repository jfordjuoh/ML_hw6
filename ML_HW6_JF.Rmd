---
title: "MLHW6_JF"
date: February 23, 2022
output: word_document
editor_options:
  chunk_output_type: console
---

```{r libraries, include=FALSE}
library(tidyverse)
library(NHANES)
library(dplyr)
library(readxl)
library(knitr)
library(Amelia)
library(e1071)
library(rpart)
library(caret)
library(rpart.plot)
library(pROC)
```

## Q1: Restrict the NHANES data to the list of 11 variables below. Partition the data into training and testing using a 70/30 split. REMINDER: Look at the frequency of your outcome variable to check for balance
```{r data cleaning and parting}
data(NHANES)

NH = NHANES %>%
  janitor::clean_names() %>%
  select(age, race1, education, hh_income, weight, height, pulse, diabetes, bmi, phys_active,  smoke100)

NH$diabetes <- factor(NH$diabetes, levels = c("No", "Yes"))

#Check distributions, missing data etc, omitting the NAs
summary(NH)
missmap(NH, main = "Missing values vs observed")
NH <- na.omit(NH)
summary(NH$diabetes) #Notice that the data is unbalanced so we will downsize

#tidyverse way to create data partition
train_indices <- createDataPartition(y = NH$diabetes,p = 0.7,list = FALSE)
train_data <- NH[train_indices, ]
test_data <- NH[-train_indices, ]

```

## Q2:Construct three prediction models to predict diabetes using the 11 features from NHANES. You will use the following three algorithms to create your prediction models: Q3: You will optimize each model using cross-validation to choose hyperparameters in the training data and then compare performance across models.

```{r Classification_Tree}
#The diabetes data is unbalanced so we will downsize.

set.seed(100)
#Creating 10-fold cross-validation and using down-sampling because of imbalance in data
train_control_ct <- trainControl(method = "cv", number = 10, sampling = "down")

#Create sequence of cp parameters to try 
grid_ct <- expand.grid(cp = seq(0.001, 0.3, by = 0.01))

#Train model
ct_diabetes <- train(diabetes~., data = train_data, method = "rpart",trControl = train_control_ct, tuneGrid = grid_ct)

ct_diabetes$bestTune #cp:0.001
ct_diabetes
rpart.plot(ct_diabetes$finalModel)

#variable importance on the final model within training data
varImp(ct_diabetes) # top three most important variables were age(100), bmi(81.99), weight(65.090) and the least important variable was income.  

#accuracy metric and confusion matrix from training.
confusionMatrix(ct_diabetes) #Accuracy(average): 0.7171

#if this the measure you choose then I can predictions on the test data as probabilities  and/or produce a ROC curve.
```

```{r Support_Vector_Classifier}
modelLookup("svmLinear")

set.seed(100)

#Set 10-fold cross-validation. Note if you want predicted probabilities, you need to set class Probs=True
train_controlSVC <- trainControl(method = "cv", number = 10, classProbs = T)

svm <- train(diabetes ~ ., data = train_data, method = "svmLinear", trControl = train_controlSVC, preProcess = c("center", "scale"))

svm #accuracy: 0.8961

#Incorporate different values for cost paramter(cp) bc this method won't tune the hyperparameters for us. The CP is how much misclassification the support vector will allow
svm_caret <- train(diabetes ~ ., data = train_data, method = "svmLinear",  trControl = train_controlSVC, preProcess = c("center", "scale"), tuneGrid = expand.grid(C = seq(0.001,2, length = 30)))

#Visualize accuracy versus values of C. This shows how the accuracy changes based on the level of the cost I chose. 
plot(svm_caret)

#Obtain metrics of accuracy from training
confusionMatrix(svm_caret) #Accuracy (average) : 0.8962
```


```{r logistic_regression}
logistic_control <- trainControl(method = "cv", number = 10, classProbs = T)

set.seed(100)
logistic <- train(diabetes ~ ., data = train_data, method = "glm", family = "binomial", trControl = logistic_control)

summary(logistic)

confusionMatrix(logistic) #Accuracy (average) : 0.8926

```


## Q4: Select a "optimal" model and calculate final evaluation metrics in the test set.

```{r Support_Vector_Classifier continued}
#Checking out info about final model
svm_caret$finalModel

#Make predictions in testset
svm_pred_test <- predict(svm_caret, test_data)

#Get evaluation metrics from test set
confusionMatrix(svm_pred_test, test_data$diabetes, positive = "No") #Accuracy 0.8966 #Sensitivity:1.00 #Specificity:0.00

#Create ROC Curve for Analysis
pred.prob <- predict(svm_caret, test_data, type = "prob")

#Another potential evaluation: Area under the Reciver Operating Curve (AUROC)
analysis <- roc(response = test_data$diabetes, predictor = pred.prob[,2])
plot(1 - analysis$specificities,analysis$sensitivities,type = "l",
ylab = "Sensitivity",xlab = "1-Specificity",col = "black",lwd = 2,
main = "ROC Curve for Diabetes Classification")
abline(a = 0,b = 1)

```


## Q5:  List and describe at least two limitations/considerations of the model generated by this analysis. Limitations can be analytical or they can be considerations that need to be made regarding how the model would be applied in practice.
Some advantages of support vector classification is that it is very effective with high dimensional data and it can be used for both regression and classification problem. Some disadvantages of this algorithm type is that on large a data set can comparatively take more time to train. SVC is not a probabilistic model so we can not explain the classification in terms of probability.