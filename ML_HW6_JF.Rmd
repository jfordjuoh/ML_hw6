---
title: "MLHW6_JF"
date: February 23, 2022
output: word_document
editor_options:
  chunk_output_type: console
---

```{r libraries, include=FALSE}
library(tidyverse)
library(NHANES)
library(dplyr)
library(readxl)
library(knitr)
library(Amelia)
library(e1071)
library(rpart)
library(caret)
library(rpart.plot)
library(pROC)
```

## Q1: Restrict the NHANES data to the list of 11 variables below. Partition the data into training and testing using a 70/30 split. REMINDER: Look at the frequency of your outcome variable to check for balance
```{r data cleaning and parting}
data(NHANES)

NH = NHANES %>%
  janitor::clean_names() %>%
  select(age, race1, education, hh_income, weight, height, pulse, diabetes, bmi, phys_active,  smoke100)

NH$diabetes <- factor(NH$diabetes, levels = c("No", "Yes"))

#Check distributions, missing data etc, omitting the NAs
summary(NH)
missmap(NH, main = "Missing values vs observed")
NH <- na.omit(NH)
summary(NH$diabetes) #Notice that the data is unbalanced so we will downsize

#tidyverse way to create data partition
train_indices <- createDataPartition(y = NH$diabetes,p = 0.7,list = FALSE)
train_data <- NH[train_indices, ]
test_data <- NH[-train_indices, ]

```

## Q2:Construct three prediction models to predict diabetes using the 11 features from NHANES. You will use the following three algorithms to create your prediction models: Q3: You will optimize each model using cross-validation to choose hyperparameters in the training data and then compare performance across models.

```{r CLASSIFICATION_TREE}
#The diabetes data is unbalanced so we will downsize.

set.seed(100)
#Creating 10-fold cross-validation and using down-sampling because of imbalance in data
train_control_ct <- trainControl(method = "cv", number = 10, sampling = "down")

#Create sequence of cp parameters to try 
grid_ct <- expand.grid(cp = seq(0.001, 0.3, by = 0.01))

#Train model
ct_diabetes <- train(diabetes~., data = train_data, method = "rpart",trControl = train_control_ct, tuneGrid = grid_ct)

ct_diabetes$bestTune #cp:0.001
ct_diabetes
rpart.plot(ct_diabetes$finalModel)

#variable importance on the final model within training data
varImp(ct_diabetes) 
# top three most important variables were age(100), bmi(81.99), weight(65.090) and the least important variable was income.  

#accuracy metric and confusion matrix from training.
confusionMatrix(ct_diabetes) #Accuracy(average): 0.7171

#if this the measure you choose then I can predictions on the test data as probabilities  and/or produce a ROC curve.
```

The accuracy of the classification tree model is 71%. 

```{r Support_Vector_Classifier}
modelLookup("svmLinear")

set.seed(100)

#Set 10-fold cross-validation. Note if you want predicted probabilities, you need to set class Probs=True
train_controlSVC <- trainControl(method = "cv", number = 10, sampling = "down", classProbs = T)

svm <- train(diabetes ~ ., data = train_data, method = "svmLinear", trControl = train_controlSVC, preProcess = c("center", "scale"))

svm #accuracy: 0.7236
```

```{r Support_Vector_Classifier continued and hiding the outcome, results = 'hide'}
#Incorporate different values for cost parameter(cp) bc this method won't tune the hyperparameters for us. The CP is how much misclassification the support vector will allow
svm_caret <- train(diabetes ~ ., data = train_data, method = "svmLinear",  trControl = train_controlSVC, preProcess = c("center", "scale"), tuneGrid = expand.grid(C = seq(0.001,2, length = 30)))
```

```{r Support_Vector_Classifier continued}
#Visualize accuracy versus values of C. This shows how the accuracy changes based on the level of the cost I chose. 
plot(svm_caret)

#Obtain metrics of accuracy from training
confusionMatrix(svm_caret) #Accuracy (average) : 0.7364
```

The accuracy of the SVC model is 73.64%. 

```{r LOGISTIC_REGRESSION}

set.seed(100)
logit <- train(
  diabetes ~ ., data = train_data, method = "glm", family = "binomial", trControl = trainControl(method = "cv", number = 10, sampling = "down"), preProc = c("center", "scale"))

logit
logit$results
confusionMatrix(logit) #Accuracy (average) : 0.7348
```
The accuracy of the logistic regression model is 73.48%. 

## Q4: Select a "optimal" model and calculate final evaluation metrics in the test set.
The optimal model I selected was the support vector classifier model because the accuracy was the highest out of the three models I've constructed.

```{r SVC_optimal_model}
#Checking out info about final model
svm_caret$finalModel

#Make predictions in testset
svm_pred_test <- predict(svm_caret, test_data)

#Get evaluation metrics from test set
confusionMatrix(svm_pred_test, test_data$diabetes, positive = "Yes") #Accuracy 0.7219 #Sensitivity:0.731 #Specificity:0.721

#Create ROC Curve for Analysis
pred.prob <- predict(svm_caret, test_data, type = "prob")

#Another potential evaluation: Area under the Receiver Operating Curve (AUROC)
#The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 â€“ FPR). Classifiers that give curves closer to the top-left corner indicate a better performance. The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.
analysis <- roc(response = test_data$diabetes, predictor = pred.prob[,2])
plot(1 - analysis$specificities,analysis$sensitivities,type = "l",
ylab = "Sensitivity",xlab = "1-Specificity",col = "black",lwd = 2,
main = "ROC Curve for Diabetes Classification")
abline(a = 0,b = 1)

```
The accuracy of the SVC model on the test data was 72.19% with a sensitivity of 0.731 and a specificity of 0.721.

## Q5:  List and describe at least two limitations/considerations of the model generated by this analysis. Limitations can be analytical or they can be considerations that need to be made regarding how the model would be applied in practice.
Some advantages of support vector classification is that it is very effective with high dimensional data and it can be used for both regression and classification problem. Some disadvantages of this algorithm type is that took more time to train the data, which is most likely due to this being a data set. SVC is not a probabilistic model so we can not explain the diabetes classification in terms of probability, which could be an issue if we wanted to use this model to explain the predictions to those who do not have a machine learning background. Essentially we would have issues with interpretability and generalizability as well. Also, this may not be applicable to small populations as they may not behave the same way. 